{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "bsize = 64\n",
    "train_dataloader = DataLoader(training_data, batch_size=bsize)\n",
    "test_dataloader = DataLoader(test_data, batch_size=bsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"data/model_1.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set hypm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"test error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------------------------------\n",
      "loss: 0.606027 [    0/60000]\n",
      "loss: 0.675668 [ 6400/60000]\n",
      "loss: 0.574102 [12800/60000]\n",
      "loss: 0.820726 [19200/60000]\n",
      "loss: 0.669711 [25600/60000]\n",
      "loss: 0.826252 [32000/60000]\n",
      "loss: 0.659233 [38400/60000]\n",
      "loss: 0.860077 [44800/60000]\n",
      "loss: 0.787595 [51200/60000]\n",
      "loss: 0.777566 [57600/60000]\n",
      "test error: \n",
      " Accuracy: 76.5%, Avg loss: 0.011320 \n",
      "\n",
      "Epoch 2\n",
      "----------------------------------------\n",
      "loss: 0.601155 [    0/60000]\n",
      "loss: 0.672516 [ 6400/60000]\n",
      "loss: 0.570776 [12800/60000]\n",
      "loss: 0.815864 [19200/60000]\n",
      "loss: 0.665578 [25600/60000]\n",
      "loss: 0.823287 [32000/60000]\n",
      "loss: 0.654921 [38400/60000]\n",
      "loss: 0.859634 [44800/60000]\n",
      "loss: 0.785470 [51200/60000]\n",
      "loss: 0.772806 [57600/60000]\n",
      "test error: \n",
      " Accuracy: 76.6%, Avg loss: 0.011273 \n",
      "\n",
      "Epoch 3\n",
      "----------------------------------------\n",
      "loss: 0.596671 [    0/60000]\n",
      "loss: 0.669569 [ 6400/60000]\n",
      "loss: 0.567952 [12800/60000]\n",
      "loss: 0.811250 [19200/60000]\n",
      "loss: 0.661728 [25600/60000]\n",
      "loss: 0.820144 [32000/60000]\n",
      "loss: 0.650332 [38400/60000]\n",
      "loss: 0.858833 [44800/60000]\n",
      "loss: 0.782628 [51200/60000]\n",
      "loss: 0.768313 [57600/60000]\n",
      "test error: \n",
      " Accuracy: 76.6%, Avg loss: 0.011229 \n",
      "\n",
      "Epoch 4\n",
      "----------------------------------------\n",
      "loss: 0.592142 [    0/60000]\n",
      "loss: 0.666636 [ 6400/60000]\n",
      "loss: 0.564908 [12800/60000]\n",
      "loss: 0.806799 [19200/60000]\n",
      "loss: 0.657614 [25600/60000]\n",
      "loss: 0.817748 [32000/60000]\n",
      "loss: 0.645828 [38400/60000]\n",
      "loss: 0.858453 [44800/60000]\n",
      "loss: 0.780202 [51200/60000]\n",
      "loss: 0.763944 [57600/60000]\n",
      "test error: \n",
      " Accuracy: 76.6%, Avg loss: 0.011188 \n",
      "\n",
      "Epoch 5\n",
      "----------------------------------------\n",
      "loss: 0.587774 [    0/60000]\n",
      "loss: 0.664138 [ 6400/60000]\n",
      "loss: 0.561837 [12800/60000]\n",
      "loss: 0.802390 [19200/60000]\n",
      "loss: 0.653434 [25600/60000]\n",
      "loss: 0.815526 [32000/60000]\n",
      "loss: 0.641706 [38400/60000]\n",
      "loss: 0.857726 [44800/60000]\n",
      "loss: 0.777987 [51200/60000]\n",
      "loss: 0.759748 [57600/60000]\n",
      "test error: \n",
      " Accuracy: 76.7%, Avg loss: 0.011149 \n",
      "\n",
      "Epoch 6\n",
      "----------------------------------------\n",
      "loss: 0.583508 [    0/60000]\n",
      "loss: 0.661647 [ 6400/60000]\n",
      "loss: 0.558976 [12800/60000]\n",
      "loss: 0.798246 [19200/60000]\n",
      "loss: 0.649596 [25600/60000]\n",
      "loss: 0.813396 [32000/60000]\n",
      "loss: 0.637690 [38400/60000]\n",
      "loss: 0.857205 [44800/60000]\n",
      "loss: 0.775715 [51200/60000]\n",
      "loss: 0.755777 [57600/60000]\n",
      "test error: \n",
      " Accuracy: 76.8%, Avg loss: 0.011112 \n",
      "\n",
      "Epoch 7\n",
      "----------------------------------------\n",
      "loss: 0.579430 [    0/60000]\n",
      "loss: 0.659496 [ 6400/60000]\n",
      "loss: 0.556229 [12800/60000]\n",
      "loss: 0.794226 [19200/60000]\n",
      "loss: 0.645773 [25600/60000]\n",
      "loss: 0.811327 [32000/60000]\n",
      "loss: 0.633623 [38400/60000]\n",
      "loss: 0.856808 [44800/60000]\n",
      "loss: 0.773493 [51200/60000]\n",
      "loss: 0.751725 [57600/60000]\n",
      "test error: \n",
      " Accuracy: 76.9%, Avg loss: 0.011077 \n",
      "\n",
      "Epoch 8\n",
      "----------------------------------------\n",
      "loss: 0.575313 [    0/60000]\n",
      "loss: 0.657374 [ 6400/60000]\n",
      "loss: 0.553411 [12800/60000]\n",
      "loss: 0.790160 [19200/60000]\n",
      "loss: 0.642136 [25600/60000]\n",
      "loss: 0.809255 [32000/60000]\n",
      "loss: 0.629868 [38400/60000]\n",
      "loss: 0.856227 [44800/60000]\n",
      "loss: 0.771524 [51200/60000]\n",
      "loss: 0.748250 [57600/60000]\n",
      "test error: \n",
      " Accuracy: 76.9%, Avg loss: 0.011043 \n",
      "\n",
      "Epoch 9\n",
      "----------------------------------------\n",
      "loss: 0.571279 [    0/60000]\n",
      "loss: 0.655285 [ 6400/60000]\n",
      "loss: 0.550814 [12800/60000]\n",
      "loss: 0.786373 [19200/60000]\n",
      "loss: 0.638062 [25600/60000]\n",
      "loss: 0.807279 [32000/60000]\n",
      "loss: 0.626141 [38400/60000]\n",
      "loss: 0.855244 [44800/60000]\n",
      "loss: 0.769734 [51200/60000]\n",
      "loss: 0.745070 [57600/60000]\n",
      "test error: \n",
      " Accuracy: 77.1%, Avg loss: 0.011010 \n",
      "\n",
      "Epoch 10\n",
      "----------------------------------------\n",
      "loss: 0.567408 [    0/60000]\n",
      "loss: 0.653280 [ 6400/60000]\n",
      "loss: 0.548247 [12800/60000]\n",
      "loss: 0.783311 [19200/60000]\n",
      "loss: 0.633852 [25600/60000]\n",
      "loss: 0.805039 [32000/60000]\n",
      "loss: 0.622641 [38400/60000]\n",
      "loss: 0.853508 [44800/60000]\n",
      "loss: 0.767771 [51200/60000]\n",
      "loss: 0.742125 [57600/60000]\n",
      "test error: \n",
      " Accuracy: 77.2%, Avg loss: 0.010980 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n----------------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 3, 3], expected input[64, 1, 28, 28] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/heng/michael bad/torch-learn/retraining.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heng/michael%20bad/torch-learn/retraining.ipynb#ch0000011?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heng/michael%20bad/torch-learn/retraining.ipynb#ch0000011?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m----------------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/heng/michael%20bad/torch-learn/retraining.ipynb#ch0000011?line=3'>4</a>\u001b[0m     train_loop(train_dataloader, model_vgg, loss_fn, optimizer)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heng/michael%20bad/torch-learn/retraining.ipynb#ch0000011?line=4'>5</a>\u001b[0m     test_loop(test_dataloader, model_vgg, loss_fn)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heng/michael%20bad/torch-learn/retraining.ipynb#ch0000011?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/heng/michael bad/torch-learn/retraining.ipynb Cell 8'\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heng/michael%20bad/torch-learn/retraining.ipynb#ch0000007?line=1'>2</a>\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heng/michael%20bad/torch-learn/retraining.ipynb#ch0000007?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/heng/michael%20bad/torch-learn/retraining.ipynb#ch0000007?line=3'>4</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heng/michael%20bad/torch-learn/retraining.ipynb#ch0000007?line=4'>5</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heng/michael%20bad/torch-learn/retraining.ipynb#ch0000007?line=6'>7</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/enter/envs/torch/lib/python3.9/site-packages/torchvision/models/vgg.py:49\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torchvision/models/vgg.py?line=47'>48</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torchvision/models/vgg.py?line=48'>49</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[1;32m     <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torchvision/models/vgg.py?line=49'>50</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[1;32m     <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torchvision/models/vgg.py?line=50'>51</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/conv.py:446\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=444'>445</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=445'>446</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/conv.py:442\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=437'>438</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=438'>439</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=439'>440</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=440'>441</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=441'>442</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    <a href='file:///Users/heng/enter/envs/torch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=442'>443</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[64, 1, 28, 28] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n----------------------------------------\")\n",
    "    train_loop(train_dataloader, model_vgg, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model_vgg, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "665c9362c857ab9fab2bd3645e927e21c39961eb9c03ec3f963d4db5fcabf320"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
