{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build standard Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        'take in and process masked src and target sequences'\n",
    "        return self.decoder(self.encoder(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define standard linear + softmax generation step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "produce N identical layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        'Core encoder is a stacl of N layers'\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        'Pass the input and mask through each layer in turn'\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "employ a residual connection around each of the two sub-layers, followed by layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    'Construct a layernorm module'\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.parameter(torch.ones(features))\n",
    "        self.b_2 = nn.parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    'A residual connection followed by a layer norm'\n",
    "    'Note for code simplisity the norm is first as opposed to last'\n",
    "\n",
    "def __init__(self, size, dropout):\n",
    "    super(SublayerConnection, self).__init__()\n",
    "    self.norm = LayerNorm(size)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "def forward(self, x, sublayer):\n",
    "    'Apply residual connection to any sublayer with the same size'\n",
    "    return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    'Encoder is made up of self-attn and feed forward'\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        'Follow Figure 1 for connections'\n",
    "        'Each layer has two sup-layers: 1.multihead self-attention 2.simple position-wisw fully connected feed-forward'\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    'Generic N layer decoder with masking'\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder is made of self-attn, src-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size,dropout),3)\n",
    "\n",
    "    \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    'Mask out subsequence position'\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAE5CAYAAAAQtqIuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAXLklEQVR4nO3df7BkZX3n8fdnsm6kYJnhx05QqA2LVIEGsj+QXTEQJiKJlQQcEWMoIwT+2FKpJLUJ4i+IA6IpE61AYqqiGxMIsYQiiggOBSwwAlbG0oirgI6IZAICovMLQSQM890/um9ybfre2/e5p/vemft+VXWde59zntPf7ur5zHN+9HNTVUiS5mfFYhcgSbsjw1OSGhiektTA8JSkBnt0eCbZnGTzYtchafczV35kT77anmQXEGDHYtciabezEqiqGjrIXBbhuXLf+Q+wn3rip7ovSNJuYyfPwizh+e+6fsIk+wAfAN4ArALuBS6uqs+O0PclwIeBX6J3SuFO4Lyquq+xnCdW7rti5dZNh82746+8+L82PqWkPcGGuo6dPPvETOvHcc7zWuBNwAXArwH3Adcm+dXZOiVZTS8sDwXOAs4A9gc+n+SQMdQpSc06HXn2A/LVwGlVdW2/7XbgMHojyvWzdD8P2A94eVU90u/7D8CDwHuAt3ZZqyQtRNcjz9fRuzhz3VRD9U6qXgEcmeRlc/S9ZSo4+323ANcDp3VcpyQtSNfheRRwX1XtGmj/2rT1z5NkL+AlwD1DVn8NWN0/rB/st322B72rZZLUua7D8wBg65D2rdPWD7MfvVuKWvpK0sR1frUdmO3ep7nui5pX36paNdvOHH1KGpeuR55bGD5C3L+/HDayBNhGLxxb+krSxHUdnvcCL00yuN+j+8th5zSpqqeB7zD8nOjRwPer6vHOqpSkBeo6PK+ld2P8KQPtZwKb5rjZ/Vrg5CQHTTUk2b+/r093XKckLUjX4bkeuB34eJJzkvxSksuB44G3T22UZEOSwXOYH6J3m9P6JK9N8mvA54Cd9L6xJElLRqfh2b+ncy1wFb3AuxH4eXo3zV8/R9/vAScADwFXAlcD24FfrKp/7rJOSVqoPX1ikO2t321v4ffhpT1H/7vtO2a6q2ePns9TksbF8JSkBoanJDUwPCWpgeEpSQ0MT0lqYHhKUgPDU5IaGJ6S1MDwlKQGhqckNTA8JanBOP4Mx7J10yNfbernhCLS7seRpyQ1MDwlqYHhKUkNDE9JamB4SlIDw1OSGhiektTA8JSkBp2GZ5KTklyeZFOSHyV5OMmnkxw9Qt91SWrI47Eua5SkLnT9DaO3AAcAfwp8A/gZ4HzgS0nWVNXGEfZxMvDktN//peMaJWnBug7Pc6vq8ekNSW4GHgTeDrx+hH18uaq2d1yXJHWq08P2weDst20H7gcO6fK5JGkxjf2CUZL/CBwF3DNil28keS7Jo0n+T5LVs+x7+2wPYGUHL0GSnmessyolCfAxeiH9oTk2fwB4N3A3vfOcv0DvfOlJSY6pqm3jrHUxtczG5ExM0uIa95R0fwKsBc6uqm/MtmFVXTnQdFuSjcDNwLnAJUP6rJptn44+JY3L2A7bk7wf+APg96rq8pZ9VNUtwKPAcR2WJkkLNpbwTHIxvUPw86vqzxa4uxXAroVXJUnd6Tw8k7wXuBC4sKr+ZIH7+mV694qOcn+oJE1Mp+c8k/wBsA64Afi/SV4xbfUzVXV3f7sNwIlVlWl97wb+FtgEPAu8EjgP+DbwF13WKUkL1fUFo1P6y1/vP6bbDBw6S99vAm8DXgy8AHgI+Cvgfd40L2mp6TQ8q2pN63ZVdUaXtUjSODmrkiQ1MDwlqYHhKUkNDE9JamB4SlKDcX+3XWPSMpkIOKGI1BVHnpLUwPCUpAaGpyQ1MDwlqYHhKUkNDE9JamB4SlIDw1OSGhiektTA8JSkBoanJDUwPCWpgeEpSQ2cVWmZcTYmqRuOPCWpQafhmWRNkprhceQI/V+S5DNJdiT5YZL1SV7WZY2S1IVxHba/A7hjoO2fZuuQZDVwJ/A4cBawE7gA+HyS/1ZVD4+hTklqMq7w/FZVbZxnn/OA/YCXV9UjAEn+AXgQeA/w1m5LlKR2S+mc5+uAW6aCE6CqtgDXA6ctWlWSNMS4wvOjSXb2z13ekOSY2TZOshfwEuCeIau/BqzuH9YP9ts+2wNY2cWLkaRBXYfnDuBS4H8BvwS8HXgZ8IUk/3OWfvsBAbYOWTfVdkB3ZUrSwnR6zrOq7gbuntZ0Z5LP0htRvh949Vy7mM+6qlo1284cfUoal7Gf86yqx4CbgVfMstk2euE4bHS5f385bFQqSYtiUheMVjDLqLKqnga+Axw1ZPXRwPer6vEx1SZJ8zb28ExyEHAyMNetS9cCJ/e3n+q7P3AK8OnxVShJ89fpOc8kn6A3gvwKvUPxI+ndML8X8K5p220ATqyqTOv+IeDNwPokF/FvN8nvBD7QZZ2StFBd3yT/deA3gd8B9ga2ABuAS6pq2G1I/6qqvpfkBHoheiW9UfGdwC9W1T93XKckLUiqZrvAvXtLsn3lvitWbt102GKXsiw5E5N2ZxvqOnby7I6Z7upZSt8wkqTdhuEpSQ0MT0lqYHhKUgPDU5IaGJ6S1MDwlKQGhqckNTA8JamB4SlJDQxPSWpgeEpSg3H96WGJmx75alM/JxTR7sCRpyQ1MDwlqYHhKUkNDE9JamB4SlIDw1OSGhiektTA8JSkBp2GZ5LLk9Qsj4Nm6btuhj6PdVmjJHWh628YvQ/4y4G2FwA3AV+rqlGC8GTgyWm//0tHtUlSZzoNz6p6AHhgeluS04C9gI+PuJsvV9X2LuuSpK5N4pznOcCPgKsn8FySNBFjnRgkyYuA1wCfqKonRuz2jSSrgceBG4D3VNXjM+x/+xz7WjlqrZI0H+OeVeks4KcY7ZD9AeDdwN30znP+AnA+cFKSY6pq29iq1JLSMhuTMzFp0sYdnr8NfLuq7phrw6q6cqDptiQbgZuBc4FLhvRZNds++yNTR5+SOje2c55JjgeOAP6mdR9VdQvwKHBcV3VJUhfGecHoHOA54IoF7mcFsGvh5UhSd8YSnkn2Bt4A3FRV313Afn4Z+BlgY1e1SVIXxnXO843APsBfD1uZZANwYlVlWtvdwN8Cm4BngVcC5wHfBv5iTHVKUpNxhefZwA+Az86jzzeBtwEvpvetpIeAvwLe503zkpaasYRnVZ0wx/o1Q9rOGEctkjQOzqokSQ0MT0lqYHhKUgPDU5IaGJ6S1GDc322XJqJlMhFwQhG1c+QpSQ0MT0lqYHhKUgPDU5IaGJ6S1MDwlKQGhqckNTA8JamB4SlJDQxPSWpgeEpSA8NTkhoYnpLUwFmVtKw5G5NaOfKUpAYjhWeSQ5JcluSuJE8mqSRrZtj25CQbkzyd5PEkH02yatSCkvxukm8leSbJA0nOT2LIS1pSRg2lw4EzgCeBW2faqB+o6+n9zfVTgPOAU4HPjRKASS4A/hS4CvgV4OPA+4EPjFinJE3EqOc876iq1QBJ1tILxGH+GLgHeGNV7epv/yhwM/AG4OqZniDJAcB7gI9U1R/2mzck2Rs4P8lHqurhEeuVpLEaaeQ5FYSzSXIwcCxw5fTtq+oW4LvA6+fYxWuAFwJXDLRfTi/kZwpsSZq4Lq+2H9Vf3jNk3denrZ+tfwH3Tm+sqvuTPD2sf5Ltc+xz5RzrJalJlxdiDugvtw5Zt3Xa+tn6/6iqnhmybtsI/SVpYsZxn2fNs33UbZ63rqpWzbaz/sjU0aekznU58tzSXw4bIe7P8BHpYP+9k/z0kHX7jdBfkiamy/CcOlc57Nzm0Qw/FzrYP8DPTW9Mcjiw1wj9JWliOgvP/m1EXwbeNP2eziQnAQcDn55jFzcCzwBvHmg/C9gJXN9VrZK0UCOf80xyev/HY/vLE5McCDxVVTf2295B757OTyb5GPBi4IPAF4Frpu1rDXA7cFFVrQOoqi1J/gi4MMmO/vrj+vu8tKoeanmBkjQO87lgdM3A7+v6y83AoQBVdVuSXwcuAj4H/BD4DHB+VT03wnNcDOwAzgXeBTwCvJdeAEvSkpGqUS6C756SbF+574qVWzcdttilSM7EtJvZUNexk2d3zHRXjxNuSFIDw1OSGhiektTA8JSkBoanJDUwPCWpgeEpSQ0MT0lqYHhKUgPDU5IaGJ6S1MDwlKQG4/gzHJKGuOmRrzb1c0KRpcmRpyQ1MDwlqYHhKUkNDE9JamB4SlIDw1OSGhiektTA8JSkBiOFZ5JDklyW5K4kTyap/t9en77NvkkuSPL5JN/rb/f/kvzvJP9+xOepGR5vmf9Lk6TxGfUbRocDZwBfAW4FTh2yzX8Cfg+4Evgw8CTwKnp/c/1EYO2Iz3U1cOlA23dG7CtJEzFqeN5RVasBkqxleHg+CBxaVU9Na7stybPAuiRHV9XXR3iux6pq44h1SdKiGOmwvap2jbDNUwPBOeVL/eUh8ylMkpaySVwwehVQwH0jbn9mkqeT/DjJF5P8xkwbJtk+2wNY2UH9kvQ8Y51VKcn/AH4HuLKqNo/Q5RPAeuAh4EXA24Crk7yoqi4bX6XS0tUyG5MzMY3f2MIzyeHAZ4Fv0gvQOVXVbw3s4++BDcAlST5WVU8PbL9qjhq24+hT0hiM5bA9yWHA7cA24OSqeqJlP/1zrX8H7AMc1V2FkrQwnYdnkv9MLzh/DJxUVY8vcJdTNc550UqSJqXT8Ezys/SC8zngVVX1yAL3twJ4E/BD4N6FVyhJ3Rj5nGeS0/s/HttfnpjkQOCpqroxyWrgNmA1cA5wcJKDp+3igar6fn9fa+iF7EVVta7fdh5wRH8fjwIHAW8FjgfOraoft7xASRqH+Vwwumbg93X95WbgUOBlwGH9tk8O6X82cPks+98EvJbeN5FWAU8B/wicWlXXz6NOSRq7kcOzqjLH+g3ArNvMtm0/IA1JSbsFZ1WSpAaGpyQ1MDwlqYHhKUkNDE9JajDWiUEkLY6WyUTACUXmw5GnJDUwPCWpgeEpSQ0MT0lqYHhKUgPDU5IaGJ6S1MDwlKQGhqckNTA8JamB4SlJDQxPSWpgeEpSA2dVkvSvnI1pdI48JanBSOGZ5JAklyW5K8mTSar/t9cHt9vQXzf4uGrUgpL8bpJvJXkmyQNJzk9iyEtaUkY9bD8cOAP4CnArcOos294PnDnQ9oNRniTJBcBFwPuB24BX9n/eH3jniLVK0tiNGp53VNVqgCRrmT08f1RVG+dbSJIDgPcAH6mqP+w3b0iyN3B+ko9U1cPz3a8kjcNIh8NVtWvchQCvAV4IXDHQfjm9kJ8tsCVposZxtf2IJNuA/wA8SC8MP1hVz87R7yiggHunN1bV/Ume7q//CUm2z7HPlaMWLUnz0XV43glcBXwT2AdYC1wMHAO8bo6+B9A75H9myLpt/fWStCR0Gp5VdeFA0w1Jvge8O8nxVXXXXLuYz7qqWjXbzvojU0efkjo3iVuAps5hHjfHdluAvZP89JB1+wFbO61KkhZgEuE59RxzXXS6Fwjwc9MbkxwO7AXc031pktRmEuE5dc/nXLcv3Qg8A7x5oP0sYCdwfcd1SVKzkc95Jjm9/+Ox/eWJSQ4EnqqqG5OcQO9G9k8Bm4G9gdcCZwPXVNUXpu1rDXA7cFFVrQOoqi1J/gi4MMmO/vrjgHcAl1bVQ60vUpK6Np8LRtcM/L6uv9wMHAo82v/9YuBAeofpm4DfB/58xOe4GNgBnAu8C3gEeC/wwXnUKUljl6rZLnDv3pJsX7nvipVbNx222KVIGrDUZ2LaUNexk2d3zHRXjxNuSFIDw1OSGhiektTA8JSkBoanJDUwPCWpgeEpSQ0MT0lqYHhKUgPDU5IaGJ6S1MDwlKQG4/gDcJI0p5se+WpTv6UyoYgjT0lqYHhKUgPDU5IaGJ6S1MDwlKQGhqckNTA8JamB4SlJDUYKzySHJLksyV1JnkxS/b+9Pn2bQ/vtMz3+coTnmanvW9peniSNx6jfMDocOAP4CnArcOqQbR4FjhvSfhbwFuAzIz7X1cClA23fGbGvJE3EqOF5R1WtBkiyliHhWVXPABsH25N8FHgYuHnE53qsqp63H0laSkY6bK+qXS07T/Jy4OeBy1v3IUlL0bgvGJ0DFPA38+hzZpKnk/w4yReT/MZMGybZPtsDWLnA+iVpqLHNqpTkhfTOk26oqlHPWX4CWA88BLwIeBtwdZIXVdVl46lU0u6kZTamcczENM4p6U4DVgF/PWqHqvqt6b8n+XtgA3BJko9V1dMD26+abX+OPiWNyzgP288BdgCfat1B/zzp3wH7AEd1VJckLdhYwjPJzwKvAj45OFpsMFWjF5wkLRnjGnmeDYR5HLIPk2QF8Cbgh8C9HdQlSZ0Y+ZxnktP7Px7bX56Y5EDgqaq6cdp2oXdj/D1V9aUZ9rUGuB24qKrW9dvOA44AbqN3w/1BwFuB44Fzq+rHI78qSRqz+Vwwumbg93X95Wbg0Gntr+r//vvzrGUT8FpgLb0LTU8B/wicWlXXz3NfkjRWI4dnVWXE7W6ld8g+2zYbBrfpB6QhKWm34KxKktTA8JSkBoanJDUwPCWpgeEpSQ3G+d12SVoSWiYT2f+I59jxxMzrHXlKUgPDU5IaGJ6S1MDwlKQGhqckNTA8JamB4SlJDQxPSWpgeEpSA8NTkhoYnpLUwPCUpAapqsWuYWyS7AKycl//j5A0Pzue2AVQVTU0QPb08NxJb3Q9bG6Ulf3ljslVtKT5fvwk34+ftBzfj32BXVU1dPa5PTo8Z5NkO0BVrVrcSpYG34+f5Pvxk3w/ns/jWUlqYHhKUgPDU5IaGJ6S1MDwlKQGhqckNTA8JanBsr3PU5IWwpGnJDUwPCWpgeEpSQ0MT0lqsOzCM8k+Sf4syaNJnk7y5SSnLnZdiyHJmiQ1w+PIxa5vnJIckuSyJHclebL/mtfMsO3JSTb2Py+PJ/loklUTLXjMRn0/kmyY4fNy1eSrXlxDp1raw10L/HfgfOBB4LeBa5OcUlXrF7OwRfQO4I6Btn9ahDom6XDgDOArwK3A0P9A+wGyHvgMcAHwYuCDwFFJTqiqXROodRJGej/67gfOHGj7wZjqWrKWVXgm+VXg1cBpVXVtv+124DDgw/T+kSxH36qqjYtdxITdUVWrAZKsZeaw+GPgHuCNU0GZ5FHgZuANwNXjL3UiRn0/AH60DD8vz7PcDttfR28y1+umGqp3o+sVwJFJXrZYhWmyRhkxJjkYOBa4cvr2VXUL8F3g9eOrcLL2oBH0xCy38DwKuG/IB+Vr09YvRx9NsjPJjiQ3JDlmsQtaIqY+D/cMWfd1lu/n5Ygk2/qfmfuTXJDkBYtd1KQtq8N24ADgW0Pat05bv5zsAC4FNtB7D14KvBP4QpITq+qLi1fakjD1edg6ZN1WeufOl5s7gauAbwL7AGuBi4Fj6B3ZLRvLLTwBZvs+6rL6rmpV3Q3cPa3pziSfpTfSej+988Oa+XOxrD4vAFV14UDTDUm+B7w7yfFVdddi1LUYltth+xaGjy737y+HjTCWlap6jN7FkFcsdi1LwJb+cqbPzLL/vPRd0V8et6hVTNhyC897gZcmGXzdR/eXw85tLUcrWIajqiHu7S+Hnds8Gj8vU6b+PS2ri07LLTyvBVYBpwy0nwlsqqr7Jl7REpPkIOBkYNnfilJVDwNfBt40/T/cJCcBBwOfXqzalpipez6X1WdmuZ3zXA/cDnw8yQH0bpI/CzgeeO1iFrYYknwC+A69G6O3AUfSu2F+L+Bdi1jaRCQ5vf/jsf3liUkOBJ6qqhv7be+gdxrjk0k+xr/dJP9F4JpJ1jtuc70fSU6gd0HxU8BmYG96/27OBq6pqi9MuubFtOzm80yyL/AB4HR6o9D7gIur6jOLWNaiSPJO4DeBQ+n9Q9hC78r7JVW1xx+SJpnpw7+5qg6dtt1rgIuA/wL8kN63jc6vqm3jrnGS5no/khwOXEbvfTiQ3mH6JnrnPP+8qp6bTKVLw7ILT0nqwnI75ylJnTA8JamB4SlJDQxPSWpgeEpSA8NTkhoYnpLUwPCUpAb/HxB/OYWTdCr9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(subsequent_mask(20)[0])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    'compute scaled dot product attention'\n",
    "    d_k = query.size(-1) # 最后一维\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim= -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "665c9362c857ab9fab2bd3645e927e21c39961eb9c03ec3f963d4db5fcabf320"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
